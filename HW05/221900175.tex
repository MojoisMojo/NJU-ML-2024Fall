%\documentclass[a4paper]{article}
%\usepackage{geometry}
%\geometry{a4paper,scale=0.8}
\documentclass[8pt]{article}
\usepackage{ctex}
\usepackage{indentfirst}
\usepackage{longtable}
\usepackage{multirow}
\usepackage[a4paper, total={6.5in, 9in}]{geometry}
\usepackage{CJK}
\usepackage[fleqn]{amsmath}
\usepackage{parskip}
\usepackage{listings}
\usepackage{fancyhdr}

\pagestyle{fancy}

% 设置页眉
\fancyhead[L]{2024年秋季}
\fancyhead[C]{机器学习}
\fancyhead[R]{作业五}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{physics}
\usepackage{tikz}
\usepackage{float}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{booktabs}

% 定义Python代码风格
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,     
    captionpos=b,        
    keepspaces=true,     
    numbers=left,        
    numbersep=5pt,       
    showspaces=false,    showstringspaces=false,
    showtabs=false,      
    tabsize=2
}

\lstset{style=mystyle}
\begin{document}

\textbf{\color{blue} \Large 姓名：毛九弢 \ \ \ 学号：221900175 \ \ \ \today}

\section*{一. (20 points) 降维}
1. 基于numpy和fetch\_lfw\_people数据集实现主成分分析（PCA）算法，不可以调用sklearn库，完成下面代码并且可视化前5个主成分所对应的特征脸(10 points)

\textbf{\large 解:}

{\color{red}代码见Prob1/task1/task1.py 和 Prob1/task1/task1Check.py, 结果见Prob1/task1/out}

可视化前5个主成分所对应的特征脸如下：
\begin{figure}[H]
    \centering
    \begin{minipage}{\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task1/out/task1_Eigenfaces.png}
        \caption{first 5 eigenfaces}
        \label{fig:task1 Eigenfaces Mine}
    \end{minipage}
    \hfill
    \begin{minipage}{\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task1/out/task1Check.png}
        \caption{first 5 eigenfaces(by sklearn as a reference)}
        \label{fig:task1 Eigenfaces Check}
    \end{minipage}
\end{figure}

2. 根据局部线性嵌入(Locally Linear Embedding, LLE)的算法流程，尝试编写LLE代码，可以基于sklearn实现，并在瑞士卷数据集上进行实验降到2维空间。提交代码和展示多个在不同参数下的可视化的实验结果。请分析使用LLE时可能遇到哪些挑战(10 points) [提示：瑞士卷数据集可以用sklearn的make\_swiss\_roll(n\_samples=3000, random\_state=0)生成3000个样本]

\textbf{\large 解:}

{\color{red}代码见Prob1/task2/task2.py, 结果见Prob1/task2/out}

(a) 原3d的瑞士卷数据集可视化如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./Prob1/task2/out/task2OriginalData.png}
    \caption{Swiss Roll}
    \label{fig:Swiss Roll}
\end{figure}

(b) 我们选取不同的近邻数目，进行LLE降维。

可以发现，近邻数目过多或过少都会导致降维后的数据不具有代表性。如果近邻数目过小，每个数据点的邻居仅限于非常局部的范围。如果近邻数目过大，每个数据点的邻居范围过于宽泛，可能包含了不相关的点。因此，我们需要选择合适的近邻数目。在这里，我们选择近邻数目为5, 7, 12, 15, 20, 30, 50, 75, 100进行降维来观察结果。

可视化结果如下：
\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_5.png}
        \caption{with 5 neighbors}
        \label{fig:Swiss Roll after LLE with 5 neighbors}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_7.png}
        \caption{with 7 neighbors}
        \label{fig:Swiss Roll after LLE with 7 neighbors}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_12.png}
        \caption{with 12 neighbors}
        \label{fig:Swiss Roll after LLE with 12 neighbors}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_15.png}
        \caption{with 15 neighbors}
        \label{fig:Swiss Roll after LLE with 15 neighbors}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_20.png}
        \caption{with 20 neighbors}
        \label{fig:Swiss Roll after LLE with 20 neighbors}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_30.png}
        \caption{with 30 neighbors}
        \label{fig:Swiss Roll after LLE with 30 neighbors}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_50.png}
        \caption{with 50 neighbors}
        \label{fig:Swiss Roll after LLE with 50 neighbors}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_75.png}
        \caption{with 75 neighbors}
        \label{fig:Swiss Roll after LLE with 75 neighbors}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{./Prob1/task2/out/check/task2LLECheck_nei_100.png}
        \caption{with 100 neighbors}
        \label{fig:Swiss Roll after LLE with 100 neighbors}
    \end{minipage}
    \hfill
\end{figure}

(c) 使用LLE时可能遇到哪些挑战:（参考资料：\href{https://www.cnblogs.com/pinard/p/6266408.html}{LLE算法详解}）

\begin{itemize} 
    \item 算法本身的局限：算法所学习的流形只能是不闭合的，且样本集是稠密均匀的。
    \item 选择合适的近邻数目：近邻数目的选择对LLE的效果有很大影响，过小的近邻数目可能导致降维后的数据不具有代表性，过大的近邻数目可能导致降维后的数据过于稀疏。
    \item 数据分布不均匀：导致局部线性假设失效，无法很好地保留数据的局部结构。
    \item 数据噪声敏感：LLE算法对数据的噪声敏感，噪声会对降维结果产生较大影响。
\end{itemize}



\vspace{3em}

\newpage
\section*{二. (20 points) 特征选择}
1. （12 points）使用 Wine 数据集，比较过滤式（基于互信息）和包裹式（RFE）特征选择方法的性能。
\begin{enumerate}[(i)]
    \item 实现两种特征选择方法，选择特征数量从1到全部特征
    \item 使用交叉验证评估不同特征数量下的模型准确率
    \item 绘制特征数量与准确率的关系图
    \item 分析并比较：两种方法的最佳特征数量和对应准确率, 计算并解释每个特征被选择的频率
\end{enumerate}

\textbf{\large 解:}

(i)\&(ii) {\color{red}代码见Prob2/task1/task1.py 和 Prob2/task2/task2.py, 结果见Prob2/out}

(iii) 特征数量与准确率的关系图如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Prob2/out/task1/feature_number_vs_accuracy.png}
    \caption{Feature Number vs Accuracy}
    \label{fig:Wine Feature Number vs Accuracy}
\end{figure}

(iv) 分析：两种方法的最佳特征数量和对应准确率, 计算并解释每个特征被选择的频率（特征重要性）

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Prob2/out/task1/task1_4.png}
    \caption{两种方法的最佳特征数量、对应准确率和每个特征被选择的频率}
    \label{fig:Two Feature Selection Methods Comparison}
\end{figure}

（文件见{\color{blue}Prob2/out/task1/task1\_4.log}）

\vspace{1em}

2.（8 points） 使用 L1 正则化的 Logistic 回归（LASSO）进行特征选择。

要求：
\begin{enumerate}[(i)]
    \item 实现基于 LASSO 的特征选择，给出代码
    \item 分析：
    \begin{enumerate}
        \item 被选择的特征（系数非零）
        \item 特征的重要性排序（基于系数绝对值大小）
        \item 基于Lasso选择出特征（对应Logistic 回归系数非0），计算对应的模型准确率
        \item 对比相同特征数量下，三种特征选择方法的模型准确率
    \end{enumerate}
\end{enumerate}

\textbf{\large 解:}

(i) {\color{red}代码见Prob2/task2/task2.py, 结果见Prob2/out}

(ii) 分析：
\begin{enumerate}[label=(\alph*)]
    \item 一共有13个特征，为0——12.
    \item 被选择的特征为：6, 11, 12, 10, 3（按照重要性排序）
    \item 重要性排序如上，系数的绝对值为：0.2653, 0.1959, 0.1517, 0.0679, 0.0674.
    \item 基于Lasso选择出特征（对应Logistic 回归系数非0），计算对应的模型准确率为0.9497.
    \item 对比相同特征数量(feature=5)下，三种特征选择方法的模型准确率如下：
    \begin{table}[H]
        \centering
        \begin{tabular}{l|c}
            \toprule
            特征选择方法 & 准确率 \\
            \midrule
            过滤式（基于互信息） & 0.96095 \\
            包裹式（RFE） & 0.96619 \\
            LASSO & 0.94968 \\
            \bottomrule
        \end{tabular}
        \caption{三种特征选择方法的模型准确率}
        \label{tab:Three Feature Selection Methods Comparison}
    \end{table}
\end{enumerate}
\vspace{3em}


\newpage
\section*{三. (20 points) 半监督}
1. 在本题中使用朴素贝叶斯模型和SST2数据集进行半监督EM算法的实践，代码前面部分如下，请补充完后续代码，只保留$10\%$的标注数据，置信度设为0.7，训练5轮，给出训练后模型在验证集上的分类结果(10 points)

\textbf{\large 解:}

{\color{red}代码见Prob3/task1.py, 结果见Prob3/out}

由于验证集数据也比较多，我们以验证集上的准确率作为模型在验证集上的分类结果。

\begin{itemize}
    \item 未使用半监督EM，模型在未标注数据上的准确率为0.8134，在验证集上的准确率为0.7775
    \item 使用半监督EM，置信度为0.7，训练5轮，模型在未标注数据上的准确率为0.7924，在验证集上的准确率为0.7695
\end{itemize}
\vspace{1em}

2. 伪标签的置信度大小对模型的训练结果会有一定的影响，通常会有固定置信度和动态设置置信度两种方式，请你完成这两种方式，并统计不同方式下每次迭代中伪标签的错误率，并分析这两种方式的优劣(5 points)

\textbf{\large 解:}

{\color{red}代码见Prob3/task1.py, 结果见Prob3/out}

\begin{itemize}
    
\item 由于错误率等于1-准确率，以下展示的是每次迭代中伪标签的准确率:（epoch 0 表示未经EM迭代时的模型准确率）

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{Prob3/out/task1_Static confidence threshold .png}
        \caption{固定置信度}
        \label{fig:Fixed Confidence}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{Prob3/out/task1_Dynamic confidence threshold +.png}
        \caption{动态置信度(每轮变大)}
        \label{fig:Dynamic Confidence plus}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\textwidth]{Prob3/out/task1_Dynamic confidence threshold -.png}
        \caption{动态置信度(每轮变小)}
        \label{fig:Dynamic Confidence minus}
    \end{minipage}
\end{figure}

\item 两种方式的优劣:
    \begin{itemize}
        \item 固定置信度：优点是简单易行。缺点是过小的固定置信度可能会导致模型在训练过程中过度自信从而过度依赖伪标签，从而影响模型的预测能力；过大的固定置信度可能会导致模型在训练中放弃过多伪标签，从而影响泛化能力。
        \item 动态置信度：优点是可以根据模型的训练情况动态调整伪标签的置信度，使得模型在训练过程中考虑伪标签的质量和数量。缺点是需要更多的调参工作，并且可能会导致模型的稳定性下降。
    \end{itemize}
\end{itemize}

\vspace{1em}

3. 修改代码，设置不同的迭代次数（如 3 次、5 次、15 次）。在验证集上分析：
不同迭代次数下，模型性能如何变化？
分析为什么在过多迭代的情况下，模型性能可能下降？(5 points)

\textbf{\large 解:}

{\color{red}代码见Prob3/task1.py, 结果见Prob3/out}

\begin{itemize}
    \item 不同迭代次数下，模型性能如下：
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{Prob3/out/task1_epochs.png}
        \caption{不同迭代次数下，模型性能}
        \label{fig:Model Performance under Different Iterations}
    \end{figure}
    \item 分析：在过多迭代的情况下，模型性能下降了。可能是由于模型在多轮迭代过程中逐步过度拟合伪标签，导致过拟合，从而导致模型的泛化能力下降，从而导致模型在验证集上的性能下降。
    \item \href{https://developer.baidu.com/article/detail.html?id=3034276}{参考资料}
\end{itemize}

\vspace{3em}

\newpage
\section*{四. (20 points) 概率图模型}

1. 证明 $a \perp\!\!\!\perp (b, c) \mid d$ 蕴含 $a \perp\!\!\!\perp b \mid d$。（5 points）

\textbf{\large 解:}

由于 $a \perp\!\!\!\perp (b, c) \mid d$ ，我们有：
$
P(a \mid b, c, d) = P(a \mid d)
$

由于条件概率的性质，得到：
$
P(a \mid b, d) = \sum_c P(a \mid b, c, d) P(c \mid b, d)
$

根据 $a \perp\!\!\!\perp (b, c) \mid d$，我们有 $P(a \mid b, c, d) = P(a \mid d)$，代入上式得到：
\[
P(a \mid b, d) = \sum_c P(a \mid d) P(c \mid b, d)
\]
由于 $P(a \mid d)$ 与 $c$ 无关，可以从求和符号中提取出来：
\[
P(a \mid b, d) = P(a \mid d) \sum_c P(c \mid b, d)
\]
根据概率的全概率公式，$\sum_c P(c \mid b, d) = 1$，因此：
\[
P(a \mid b, d) = P(a \mid d)
\]
即 $a \perp\!\!\!\perp b \mid d$。因此，$a \perp\!\!\!\perp (b, c) \mid d$ 蕴含 $a \perp\!\!\!\perp b \mid d$。

\vspace{3em}

2. 假设你有一组 $d$ 个二元随机变量 $\{X_1, ..., X_d\}$。（5 points）
\begin{enumerate}[(i)]
        \item 
        在不做任何独立性假设的情况下，完全描述联合分布所需的最小参数个数是多少？ \\
        \textit{提示:} 由于总概率之和必须为$1$，所需的参数个数比结果的总数少一个。
        
        \textbf{\large 解:} $2^d - 1$

        因为是二元随机变量，每个随机变量有两个取值，所以每个随机变量的联合分布有 $2^d$ 种可能的取值。由于提示，所以所需的最小参数个数是$2^d - 1$
        \vspace{1em}

        \item 
       假设这些随机变量的结构为马尔可夫链，其中每个 $X_i$ 仅依赖于 $X_{i-1}$。 在这种情况下，所需的最小参数个数是多少？
        
        \textbf{\large 解:} $2d - 1$

        在马尔可夫链结构中，$X_1$ 的分布需要 1 个参数（因为 $X_1$ 是二元的），每个 $X_i$ 的条件分布 $P(X_i \mid X_{i-1})$ 需要 2 个参数（因为 $X_{i-1}$ 有 2 种取值，每种取值下 $X_i$ 的条件概率需要 1 个参数）。因此，总参数个数为：
        \[
        1 + 2(d-1) = 2d - 1
        \]
        \vspace{1em}


        \item 
        从不做独立性假设到引入马尔可夫假设，参数复杂度是如何变化的？
        
        \textbf{\large 解:}

        参数复杂度从指数级$O(2^d)$（$2^d - 1$）降低为线性级$O(d)$（$2d - 1$），显著减少了参数的数量。
        \vspace{1em}
    \end{enumerate}
\vspace{3em}

3. 考虑以下三种结构各异的图模型。
\begin{center}
    \begin{tikzpicture}
        % G1 structure
        \node[circle, draw=black, minimum size=1cm] (a1) at (0, -1.5) {$a$};
        \node[circle, draw=black, minimum size=1cm] (c1) at (1.75, -1.5) {$c$};
        \node[circle, draw=black, minimum size=1cm] (b1) at (3.5, -1.5) {$b$};
        \draw[->, black] (a1) -- (c1);
        \draw[->, black] (c1) -- (b1);
        \node at (1.75, -4) {G1};
    
        % Vertical bar between G1 and G2
        \draw[thick] (4.5, -0.5) -- (4.5, -4.5);
    
        % G2 Common Cause structure
        \node[circle, draw=black, minimum size=1cm] (c2) at (7, -1.5) {$c$};
        \node[circle, draw=black, minimum size=1cm] (a2) at (5.5, -3) {$a$};
        \node[circle, draw=black, minimum size=1cm] (b2) at (8.5, -3) {$b$};
        \draw[->, black] (c2) -- (a2);
        \draw[->, black] (c2) -- (b2);
        \node at (7, -4) {G2};
    
        % Vertical bar between G2 and G3
        \draw[thick] (9.5, -0.5) -- (9.5, -4.5);
    
        % G3 Common Effect structure
        \node[circle, draw=black, minimum size=1cm] (c3) at (12, -3) {$c$};
        \node[circle, draw=black, minimum size=1cm] (a3) at (10.5, -1.5) {$a$};
        \node[circle, draw=black, minimum size=1cm] (b3) at (13.5, -1.5) {$b$};
        \draw[->, black] (a3) -- (c3);
        \draw[->, black] (b3) -- (c3);
        \node at (12, -4) {G3};
    \end{tikzpicture}
\end{center}
请将每个情境与最合适的图模型进行匹配。（5 points）
\begin{enumerate}[(i)]
        \item 
        一个家庭的旅行决定 (\(c\)) 会受到 父母的工作安排 (\(a\)) 孩子的学校假期 (\(b\))的影响。

        \textbf{\large 解:} G3
        \vspace{1em}
        
        \item 
        破纪录的大雪 (\(c\))  会同时刺激 滑雪度假村的预订量 (\(a\)) 和 冬季服装的需求 (\(b\))。

        \textbf{\large 解:} G2
        \vspace{1em}

        \item 
        个人的锻炼习惯 (\(a\)) 会影响 自身的能量水平 (\(c\))，进而影响 工作效率 (\(b\))。

        \textbf{\large 解:} G1
        \vspace{1em}

        \item 
        一个地区的气候 (\(a\)) 决定了 生长的植被类型 (\(c\))，而植被类型又会影响 野生动物的数量 (\(b\))。

        \textbf{\large 解:} G1
        \vspace{1em}

        \item 
        一个国家的经济稳定性 (\(c\)) 会影响 就业率 (\(a\)) 和 消费者的消费习惯 (\(b\))。

        \textbf{\large 解:} G2
        \vspace{1em}
        
        \item 
        餐厅的受欢迎程度 (\(c\)) 取决于 食品质量 (\(a\)) 和 社交媒体的曝光度 (\(b\))。
        
        \textbf{\large 解:} G3
        \vspace{1em}

    \end{enumerate}




4. 考虑以下有向图，其中所有变量均为未观测变量。 （5 points）

    \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \node[circle, draw=black, thick, minimum size=1cm] (a) at (-1.5, 2) {\(a\)};
            \node[circle, draw=black, thick, minimum size=1cm] (b) at (3.5, 2) {\(b\)};
            \node[circle, draw=black, thick, minimum size=1cm] (c) at (1, 1) {\(c\)};
            \node[circle, draw=black, thick, minimum size=1cm] (d) at (1, -1.5) {\(d\)};
            \draw[->, thick, black] (a) -- (c);
            \draw[->, thick, black] (b) -- (c);
            \draw[->, thick, black] (c) -- (d);
        \end{tikzpicture}
    \end{figure}

    \begin{enumerate}[(i)]
        \item 

        在给定的图中，如何将联合分布 $p(a, b, c, d)$ 表示为边际分布和条件分布的组合？
        
        \textbf{\large 解:}

        已知：$a \perp\!\!\!\perp d \mid c, b \perp\!\!\!\perp d \mid c, a \perp\!\!\!\perp b$, 故而：
        \begin{align*}
            p(a,b,c,d) 
            &=p(c,d|a,b)p(a,b) \\
            &= p(c,d|a,b)p(a)p(b)\\
            &= p(d|a,b,c)p(a)p(b)p(c) \\
            &= p(d|c)p(a)p(b)p(c)
        \end{align*}
        \vspace{1em}

        \item 

        假设 $a, b, c, d$ 是二元随机变量，求建模该联合分布所需的最小参数个数。
        
        \textbf{\large 解:} 5个参数
        
        $p(c)$需要一个，$p(b)$需要一个，$p(a)$需要一个，$p(d|c)$需要两个($c$有两个值)，所以最少要5个。
        \vspace{1em}

        \item 
        
        证明 $a$ 和 $b$ 是相互独立的，即 \(a \perp\!\!\!\perp b\)。
        
        \textbf{\large 解:}

        在贝叶斯网络中，两个节点之间的独立性可以通过图的结构来判断。具体来说，如果两个节点之间没有直接的路径，并且它们之间的所有路径都被阻断（即满足d-分离条件），那么这两个节点是相互独立的。

        在给定的图中，$a$ 和 $b$ 之间没有直接的路径。
        
        根据d-分离的定义，如果路径上的中间节点是“v-结构”（即箭头指向中间节点），并且中间节点及其所有子节点未被观测到，那么这条路径被阻断。在我们的图中，路径 $a \rightarrow c \leftarrow b, c \cdots \rightarrow d$ 是一个典型的v-结构，节点 $c$及其所有子节点({$d$}) 都是未观测节点。因此，路径 $a \rightarrow c \leftarrow b$ 被阻断。
        
        由于 $a$ 和 $b$ 之间的唯一路径被阻断，根据d-分离的定义，$a$ 和 $b$ 是相互独立的。

        \item 
        假设对于图中任意节点 $x$ , 都有 $p(x \mid \text{pa}(x)) \neq p(x)$, 其中 $\text{pa}(\cdot)$ 表示节点 $x$ 的父节点集合。
        证明当观测到 $d$ 时, $a$ 和 $b$ 不再相互独立, 即 \(a \not\perp\!\!\!\perp b \mid d\). 
        
        \textbf{\large 解:}
        
        根据d-分离的定义，如果路径上的中间节点是“v-结构”（即箭头指向中间节点），并且中间节点未被观测到，那么这条路径被阻断。在我们的图中，路径 $a \rightarrow c \leftarrow b$ 是一个典型的v-结构，节点 $c$ 是未观测节点。

        但，由于节点 $d$ 是节点 $c$ 的子节点，观测到 $d$时，由于$p(c|d)\not= p(c)$，节点 $c$ 的概率分布和未观测时的概率分布不同，从而影响节点 $a$ 和 $b$ 之间的独立性。
        
        具体来说，观测到 $d$ 会使得节点 $c$ 的状态变得相关，因为 $d$ 是 $c$ 的子节点。由于 $c$ 是 $a$ 和 $b$ 的共同子节点，观测到 $d$ 会使得 $a$ 和 $b$ 之间变得相关。因此，$a$ 和 $b$ 不再相互独立。
        
        具体反例如下：
        假设$a,b,c,d$为二元随机变量，极端地假设条件概率如下：
        \begin{table}[H]
            \centering
            \begin{tabular}{|c|c|c|}
                \hline
                $p(a|c)$ & $a=1$ & $a=0$ \\
                \hline
                $c=1$ & $0.5$ & $0.5$ \\
                \hline
                $c=0$ & $0.5$ & $0.5$ \\
                \hline
            \end{tabular}
            \caption{Conditional probabilities of $a$ given $c$}
        \end{table}
        
        \begin{table}[H]
            \centering
            \begin{tabular}{|c|c|c|}
                \hline
                $p(b|c)$ & $b=1$ & $b=0$ \\
                \hline
                $c=1$ & $0.5$ & $0.5$ \\
                \hline
                $c=0$ & $0.5$ & $0.5$ \\
                \hline
            \end{tabular}
            \caption{Conditional probabilities of $b$ given $c$}
        \end{table}
        
        \begin{table}[H]
            \centering
            \begin{tabular}{|c|c|c|}
                \hline
                $p(c|d)$ & $c=1$ & $c=0$ \\
                \hline
                $d=1$ & $0.5$ & $0.5$ \\
                \hline
                $d=0$ & $0.5$ & $0.5$ \\
                \hline
            \end{tabular}
            \caption{Conditional probabilities of $c$ given $d$}
        \end{table}
        
        \begin{table}[H]
            \centering
            \begin{tabular}{|c|c|c|c|c|}
                \hline
                $p(a,b|c)$ & $a=1, b=1$ & $a=1, b=0$ & $a=0, b=1$ & $a=0, b=0$ \\
                \hline
                $c=1$ & 0.6 & 0.2 & 0.2 & 0 \\
                \hline
                $c=0$ & 0 & 0.2 & 0.2 & 0.6 \\
                \hline
            \end{tabular}
            \caption{Joint probabilities of $a$ and $b$ given $c$}
        \end{table}
        
        \begin{table}[H]
            \centering
            \begin{tabular}{|c|c|}
                \hline
                $p(c)$ & Probability \\
                \hline
                $c=0$ & $0.3$ \\
                \hline
                $c=1$ & $0.7$ \\
                \hline
            \end{tabular}
            \caption{Marginal probabilities of $c$}
        \end{table}
       以$p(a=1,b=1|d=1)$为例：
        \begin{align*}
        p(a|d) \cdot p(b|d) 
        &= 
        \sum_{c}p(a,c|d) \cdot \sum_{c}p(b,c|d) \\
        &=\sum_{c}p(a|c,d)p(c|d) \cdot \sum_{c}p(b|c,d)p(c|d) \\
        &=\sum_{c}p(a|c)p(c|d) \cdot \sum_{c}p(b|c)p(c|d) \\
        &=0.25
        \end{align*}
        \begin{align*}
        p(a,b|d) 
        &= \sum_{c}p(a,b,c|d) \\
        &= \sum_{c}p(a,b|c,d)p(c|d) \\
        &= \sum_{c}p(a,b|c)p(c|d) \\
        &= 0.3\\
        \end{align*}
        所以$p(a,b|d)\not=p(a|d) \cdot p(b|d)$，即\(a \not\perp\!\!\!\perp b \mid d\)
        \vspace{1em}

    \end{enumerate}

\newpage
\section*{五. (20 points) 强化学习}

在本问题中，你将思考如何通过在马尔可夫决策过程（MDP）中连续做决策来最大化奖励，并深入了解贝尔曼方程——解决和理解MDP的核心方程。

考虑经典的网格世界MDP，其中智能体从单元格 (1, 1) 开始，并在环境中导航。在这个世界中，智能体每个格子里可以采取四个动作：上、下、左、右。格子用 (水平，垂直) 来索引；也就是说，单元格 (4, 1) 位于右下角。世界的转移概率如下：如果智能体采取一个动作，它将以 0.8 的概率移动到动作的方向所在的格子，并以 0.1 的概率滑到动作的相对右或左的方向。如果动作（或滑动方向）指向一个没有可通过的格子（即边界或 (2, 2) 格子的墙壁），那么该动作将保持智能体处于当前格子。例如，如果智能体在 (3, 1) 位置，并采取向上的动作，它将以 0.8 的概率移动到 (3, 2)，以 0.1 的概率移动到 (2, 1)，以 0.1 的概率移动到 (4, 1)。如果智能体在 (1, 3) 位置并采取右移动作，它将以 0.8 的概率移动到 (2, 3)，以 0.1 的概率移动到 (1, 2)，以 0.1 的概率停留在 (1, 3)。当智能体到达定义的奖励状态时（在 (4, 2) 和 (4, 3) 单元格），智能体将获得相应的奖励，并且本次回合结束。

回顾计算MDP中每个状态的\textit{最优价值}, $V^{*}(s)$的贝尔曼方程，其中我们有一组动作$A$，一组状态$S$，每个状态的奖励值$R(s)$，我们的世界的转移动态$P(s' | s, a)$，以及折扣因子$\gamma$：
\begin{align*}
V^{*}(s) = R(s) + \gamma \max_{a \in A} \sum_{s' \in S} P(s' | s, a) V^{*}(s')
\end{align*}
最后，我们将策略表示为$\pi(s) = a$其中策略$\pi$指定了在给定状态下采取的行动。

\begin{enumerate}[(a)]
\item 
考虑一个智能体从单元格 (1, 1) 开始，在第 1 步和第 2 步分别采取向上和向上的动作。计算在每个时间步内，根据这一动作序列，智能体可以到达哪些单元格，以及到达这些单元格的概率。（6 points）

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|}
    \hline
       单元格 &  概率\\
    \hline
       (1,2) & 0.8 \\
    \hline
        (1,1) & 0.1 \\
    \hline
        (2,1) & 0.1 \\
    \hline
    \end{tabular}
    \caption{第一步到达单元格的概率}
    \label{tab:first step Prob}
\end{table}

由于(2,2)有墙壁，故而Step 2：
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|}
    \hline
       单元格 &  概率\\
    \hline
       (1,3) & 0.64 \\
    \hline
        (1,2) & 0.24 \\
    \hline
        (2,1) & 0.10 \\
    \hline
        (3,1) & 0.01 \\
    \hline
        (1,1) & 0.01 \\
    \hline
    \end{tabular}
    \caption{第二步到达单元格的概率}
    \label{tab:second step Prob}
\end{table}

\vspace{1em}

\item
考虑当前没有奖励值的所有状态的奖励函数 $R(s)$（即除了 (4, 2) 和 (4, 3) 以外的每个单元格）。 定义在以下奖励值下，智能体的最优策略： (i.) $R(s) = 0$, (ii.) $R(s) = -2.0$, and (iii.) $R(s) = 1.0$. 你可以假设折扣因子接近 1，例如 0.9999。画出网格世界并标出在每个状态下应采取的动作可能会对你有所帮助（记住，策略是在 MDP 中对所有状态进行定义的！）（7 points）

\emph{注意}: 你不需要算法上计算最优策略。你必须列出每种情况的完整策略，但只需要提供直观的理由。
        
\textbf{\large 解:}

\begin{enumerate}[label=(\roman*)]
    \item $R(s) = 0$
    \item $R(s) = -2.0$
    \item $R(s) = 1.0$
    \begin{enumerate}
        \item $R(奖励状态的奖励) > 1.0$
        \item $R(奖励状态的奖励) <= 1.0$
    \end{enumerate}
\end{enumerate}

\vspace{1em}



\item
有时，MDP 的奖励函数形式为 $R(s, a)$ 它依赖于所采取的动作，或者奖励函数形式为 $R(s, a, s')$ ，它还依赖于结果状态。写出这两种形式的最优价值函数的贝尔曼方程。（7 points）
        
\textbf{\large 解:}
\vspace{3em}



\end{enumerate}

\end{document}
