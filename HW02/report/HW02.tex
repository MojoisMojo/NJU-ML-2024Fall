%\documentclass[a4paper]{article}
%\usepackage{geometry}
%\geometry{a4paper,scale=0.8}
\documentclass[8pt]{article}
\usepackage{ctex}  % 支持中文
\usepackage{indentfirst}  % 首行缩进
\usepackage{longtable}  % 支持长表格
\usepackage{multirow}  % 支持多行单元格
\usepackage[a4paper, 
top=1in, bottom=1in, left=0.8in, right=0.8in,
]{geometry}  % 设置页面尺寸和边距
\usepackage{CJK}  % 支持中文
\usepackage[fleqn]{amsmath}
\usepackage{parskip}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{diagbox}
\pagestyle{fancy}

% 设置页眉
\fancyhead[L]{2024年秋季}
\fancyhead[C]{机器学习}
\fancyhead[R]{作业二}


\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}

% 定义Python代码风格
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,     
    captionpos=b,        
    keepspaces=true,     
    numbers=left,        
    numbersep=5pt,       
    showspaces=false,    showstringspaces=false,
    showtabs=false,      
    tabsize=2
}

\lstset{style=mystyle}
\begin{document}

\textbf{\color{blue} \Large 姓名：毛九弢 \ \ \ 学号：221900175 \ \ \ \today}

\section*{零. 实验环境}
\begin{itemize}
    \item 操作系统：Windows 11
    \item Python 版本：3.9.19
    \item 依赖库：见requirements.txt
\end{itemize}

\section*{一. (30 points) 类别不平衡 [本题题面对正负样本的描述描述反了]}

信用卡欺诈检测数据集 (Credit Card Fraud Detection)包含了2013年9月通过信用卡进行的欧洲持卡人的交易。这是一个非常典型的类别不平衡数据集，数据集中正常交易的标签远多于欺诈交易。请你根据附件中提供的该数据集完成以下问题：

参考链接: 
\url{https://zhuanlan.zhihu.com/p/134091240},
\url{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC},
\url{https://www.cnblogs.com/linjingyg/p/15708635.html}.

\subsection*{代码说明}
\begin{itemize}
    \item \textcolor{red}{代码见压缩包code中的Prob1, 详细说明见Prob1中的README文件}; 
    \item 详细输出见Prob1中的out文件夹。
\end{itemize}

\subsection*{Task1}
该数据集共有284807个样本，其中只有492个负样本。请按照训练集和测试集比例4：1的方式划分数据集（使用固定的随机种子）。在训练集上训练SVM模型，并计算该模型在测试集上的精度（如准确率、召回率、F1分数，AUC等）。请展示SVM模型训练过程的完整代码，并绘制ROC曲线（8 points）；

{代码见压缩包code中的Prob1, 详细说明见Prob1中的README文件}

\begin{minipage}{\textwidth}
    \begin{minipage}{0.65\textwidth}
        \begin{table}[H]
            \caption{SVM模型性能}
            \raggedright
            \begin{tabular}{cccccc}
            \toprule
            指标 & ACC & PRE & REC & F1 & AUC \\
            \midrule
            SVM & 0.999350 & 0.906667 & 0.693878 & 0.786127 & 0.968045\\
            \bottomrule
            \end{tabular}
        \end{table}
    \end{minipage}
    \begin{minipage}{0.35\textwidth}
        \begin{figure}[H]
            \raggedright
            \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task1/roc_curve.png}
            \caption{ROC曲线}
        \end{figure}
    \end{minipage}
\end{minipage}
\subsection*{Task2}
请从上述训练集中的正样本中分别随机剔除2000，20000，200000个样本，剩余的正样本和训练集中原本的负样本共同组成新的训练集，测试集保持不变。请参照上一小问方式在这三个新的训练集上训练svm模型，并记录每个模型的精度。观察并比较这几组实验的结果，结合准确率与召回率的定义，请说明不平衡数据集对模型的影响（9 points）；
\subsection*{2.1 结果}
\begin{table}[H]
    \centering
    \caption{剔除不同数目样本的SVM模型性能}
    \begin{tabular}{cccccc}
    \toprule
    指标 & ACC & PRE & REC & F1 & AUC \\
    \midrule
    SVM & 0.999350 & 0.906667 & 0.693878 & 0.786127 & 0.968045\\
    \midrule
    SVM-rm2k & 0.999350 & 0.906667 & 0.693878 & 0.786127 & 0.968553\\
    \midrule
    SVM-rm2w & 0.999368 & 0.907895 & 0.704082 & 0.793103 & 0.969577\\
    \midrule
    SVM-rm20w & 0.999298 & 0.802083 & 0.785714 & 0.793814 & 0.973112\\
    \bottomrule
    \end{tabular}
\end{table}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.55\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task2/Metrics_curve_rc_0_2k_2w_20w.png}
        \label{fig:Metrics_curve_rc_0_2k_2w_20w}
        \caption{变化曲线}
    \end{minipage}
\end{figure}
剔除不同数目样本的ROC曲线
\begin{figure}[H]
    \centering
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task2/roc_curve_remove2000.png}
        \caption{剔除2000个样本}
        \label{fig:roc_curve_remove2000}
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task2/roc_curve_remove20000.png}
        \caption{剔除20000个样本}
        \label{fig:roc_curve_remove20000}
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task2/roc_curve_remove200000.png}
        \caption{剔除200000个样本}
        \label{fig:roc_curve_remove200000}
    \end{minipage}
    \hfill
\end{figure}

\subsection*{2.2 回答和结论}

PRE 和 REC 的定义：
\(      \text{ \ \ PRE} = \frac{\text{TP}}{\text{TP+FP}}, 
        \text{REC} = \frac{\text{TP}}{\text{TP+FN}}
\)

以这个负样本更多的数据集来说，在不进行任何优化处理的情况下，模型在模型没把握确定的时候，更加偏向预测为负样本（因为这样误差期望小），所以 REC 会低；换句话来说，如果模型预测了一个样本为正样本，那么模型一定是有很大把握的， 所以 PRE 会高。

\textbf{\textcolor{red}{简单来说：}}
\begin{itemize}
    \item 一个数据集中负样本更多，模型更容易预测为负样本，这样会导致 PRE 高，REC 低。
    \item 一个数据集中正样本更多，模型更容易预测为正样本，这样会导致 PRE 低，REC 高。
\end{itemize}

\subsection*{Task3 实现SMOTE算法中的over sampling函数, 以代码块的形式附于下方即可.}
除了上述第2问的随机欠采样的方式以外，对小类样本的“过采样”也是处理不平衡问题的基本策略。一种经典的方法为人工合成的过采样技术(Synthetic Minority Over-sampling Technique, SMOTE), 其在合成样本时寻找小类中某一个样本的近邻, 并在该样本与近邻之间进行差值, 作为合成的新样本。（8 points）；
\begin{lstlisting}[breaklines=true, language=Python, caption=过采样函数实现]
def over_sampling(self):
    N = self.N # N 为每个样本生成的合成样本数量
    n_synthetic_samples = N * self.n_sample # 计算需要生成的合成样本数量
    synthetic_samples = np.zeros((n_synthetic_samples, self.n)) # 初始化合成样本数组

    # 使用 K 近邻算法找到每个样本的 K 个最近邻
    neigh = NearestNeighbors(n_neighbors=self.K)
    neigh.fit(self.sample)
    neighbors = neigh.kneighbors(self.sample, return_distance=False)

    for i in mytqdm(range(self.n_sample), desc="Generating synthetic samples"):
        for n in range(N):
            nn = np.random.choice(neighbors[i][neighbors[i] != i])
            diff = self.sample[nn] - self.sample[i]
            gap = np.random.rand()
            synthetic_samples[i * N + n] = self.sample[i] + gap * diff
    return synthetic_samples, np.ones(n_synthetic_samples) * self.label
\end{lstlisting}

\subsection*{Task4 请说明SMOTE算法的缺点并讨论可能的改进方案（5 points）。}
我的实现中，设置K近邻的K为7，并调整不同的N值，分别为3, 5, 7, 15, 30, 50, 100, 200, 400，观察并比较不同N值对模型性能的影响.
\subsection*{4.1 结果如下:（N代表每个样本合成的样本数目）}
\begin{table}[H]
    \centering
    \caption{不同N的SVM模型性能}
    \begin{tabular}{cccccc}
    \toprule
    指标 & ACC & PRE & REC & F1 & AUC \\
    \midrule
    SVM & 0.999350 & 0.906667 & 0.693878 & 0.786127 & 0.968045\\
    \midrule
    SVM-N3 & 0.999456 & 0.913580 & 0.755102 & 0.826816 & 0.968986\\
    \midrule
    SVM-N5 & 0.999544 & 0.918605 & 0.806122 & 0.858696 & 0.975347(感觉不正常)\\
    \midrule
    SVM-N7 & 0.999544 & 0.918605 & 0.806122 & 0.858696 & 0.969755\\
    \midrule
    SVM-N15 & 0.999438 & 0.851064 & 0.816327 & 0.833333 & 0.976791\\
    \midrule
    SVM-N30 & 0.999192 & 0.728070 & 0.846939 & 0.783019 & 0.980940\\
    \midrule
    SVM-N50 & 0.998912 & 0.636364 & 0.857143 & 0.730435 & 0.981739\\
    \midrule
    SVM-N100 & 0.998227 & 0.491329 & 0.867347 & 0.627306 & 0.975618\\
    \midrule
    SVM-N200 & 0.995558 & 0.265861 & 0.897959 & 0.410256 & 0.975457\\
    \midrule
    SVM-N400 & 0.988115 & 0.116556 & 0.897959 & 0.206331 & 0.969106\\
    \bottomrule
    \end{tabular}
\end{table}
\begin{figure}[H]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/Metrics_curve_N_0_3_5_7_15_30_50_100_200_400.png}
        \label{fig:Metrics_curve_N_0_3_5_7_15_30_50_100_200_400}
        \caption{变化曲线}
    \end{minipage}
\end{figure}
不同典型 N 的ROC曲线
\begin{figure}[H]
    \centering
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/roc_curve_N_3_K_7.png}
        \caption{N=3}
        \label{fig:roc_curve_N_3_K_7}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/roc_curve_N_7_K_7.png}
        \caption{N=7}
        \label{fig:roc_curve_N_7_K_7}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/roc_curve_N_15_K_7.png}
        \caption{N=15}
        \label{fig:roc_curve_N_15_K_7}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/roc_curve_N_30_K_7.png}
        \caption{N=30}
        \label{fig:roc_curve_N_30_K_7}
    \end{minipage}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/roc_curve_N_50_K_7.png}
        \caption{N=50}
        \label{fig:roc_curve_N_50_K_7}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/roc_curve_N_100_K_7.png}
        \caption{N=100}
        \label{fig:roc_curve_N_100_K_7}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/roc_curve_N_200_K_7.png}
        \caption{N=200}
        \label{fig:roc_curve_N_200_K_7}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob1/out/rand_seed_14/task3/roc_curve_N_400_K_7.png}
        \caption{N=400}
        \label{roc_curve_N_400_K_7}
    \end{minipage}
\end{figure}

\subsection*{4.2 回答和结论}
随着 N 的增大，模型的性能逐渐下降，合成样本的质量下降，过近的合成样本造成过拟合，跨边界合成样本成为噪声，导致模型的泛化能力下降。

\textcolor{blue}{4.2.1 缺点}
    \begin{itemize}
        \item N过大K过小的合成样本可能会引入冗余信息(全部合成在某个负样本周围)，导致模型泛化能力下降。
        \item 合成样本可能会引入错误信息和噪声(比如跨边界合成样本，即两个负样本之间夹着正样本区域,但是合成样本的label却是负的)，导致模型性能下降。
        \item 将原本就比较大的训练集变得更大，导致训练时间增加。
    \end{itemize}

\textcolor{blue}{4.2.2. 可能的改进方案}
    \begin{itemize}
        \item 检查是否是跨边界合成样本，如果是则不合成，但是这样计算量会增加。
        \item 对于K近邻的"邻"进行限制，比如设置最大距离和最小距离，避免合成样本过于集中造成过拟合或过于分散造成跨类别合成。
        \item 使用ENN删除噪声样本, 以减少基于噪声样本的合成样本。
        \item 混合采样方法，即同时使用过采样和欠采样方法。
        \item 调整 K, N 的大小，使得合成样本的数量适中,质量更高。
    \end{itemize}

\vspace{3em}

\section*{二. (20 points) 机器学习中的过拟合现象}

本题以决策树和多层感知机为例, 探究机器学习中的过拟合现象。 在教材2.1 节中提到, 机器学习希望训练得到的模型在新样本上保持较好的泛化性能. 如果在训练集上将模型训练得“过好”, 捕获到了训练样本中对分类无关紧要的特性, 会导致模型难以泛化到未知样本上, 这种情况称为过拟合。

\subsection*{代码 与 数据集的选择}
\begin{itemize}
    \item \textcolor{red}{代码见压缩包code中的Prob2, 详细说明见Prob2中的README文件}; 
    \item \textcolor{red}{数据集加载见Prob2中的dataloader.py文件，详细输出见Prob2中的out文件夹}。
\end{itemize}

\subsection*{Task1 请简要总结决策树和多层感知机的工作原理及其对应的缓解过拟合的手段（5 points）} 
\subsection*{1.1 决策树}

\textcolor{blue}{1.1.1 工作原理:}
\begin{itemize}
    \item 决策树是一种基于树结构的监督学习算法。它将数据递归地划分为多个子集，目的是通过询问一系列的条件问题来实现分类或回归。每个内部节点表示一个属性，边表示属性的划分条件，叶节点表示决策结果。
    \item 而在选择划分属性时，常用的划分标准包括信息增益、基尼系数等。
\end{itemize}

\textcolor{blue}{1.1.2 缓解过拟合:}
\begin{itemize}
    \item \textbf{剪枝 (Pruning)}：剪枝可以分为预剪枝（构建过程中停止增长）和后剪枝（构建后回溯修剪）。
    \item \textbf{设置最大深度}：限制树的最大深度，防止过深的树对训练数据过拟合。
    \item \textbf{最小样本数}：设置分裂节点所需的最小样本数，避免过度划分。
    \item \textbf{随机森林 (Random Forest)}：通过集成多个决策树并对其结果取平均，可以降低单棵树的过拟合风险。
\end{itemize}

\subsection*{1.2 多层感知机}

\textcolor{blue}{1.2.1 工作原理:}
\begin{itemize}
    \item 多层感知机是一种前馈神经网络(不存在同层/跨层连接)，包含至少一个隐藏层。每一层中的神经元通过权重和偏置与下一层的神经元相连，网络通过非线性激活函数（如ReLU、Sigmoid）将输入映射到输出。
    \item 网络通过反向传播算法不断调整权重，以最小化损失函数的误差，从而实现分类或回归任务。
\end{itemize}

\textcolor{blue}{1.2.2 缓解过拟合:}
\begin{itemize}
    \item \textbf{早停(early stopping)}：
    \begin{itemize}
        \item 若训练误差连续 a 轮的变化小于 b, 则停止训练
        \item 使用验证集：若训练误差降低、验证误差升高, 则停止训练
    \end{itemize}
    \item \textbf{正则化 (regularization)}：在损失函数中加入惩罚项，描述网络复杂度。例如$(1-\lambda)\sum_{i}{w_i^2} $
    \item \textbf{Dropout}：在训练过程中随机“丢弃”部分神经元，防止网络过度依赖某些节点，增强泛化能力。
    \item \textbf{数据增强 (Data Augmentation)}：通过对训练数据进行随机变换，增加数据的多样性，从而提升模型的泛化性能。
\end{itemize}

\subsection*{Task2 请使用scikit-learn 实现决策树模型, 并扰动决策树的最大深度$max\_ depth$。一般来说, $max\_ depth$的值越大, 决策树越复杂, 越容易过拟合, 实验并比较测试集精度, 讨论并分析观察到的过拟合现象等（5 points）；}


\subsection*{2.1 实验过程和结果}

我们选取了[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 18, 22, 26, 30]作为$max\_ depth$的值，并发现在$max\_ depth$较大的时候，模型在不少数据集上出现了过拟合现象，全部图像见Prob2/out/task2文件夹中。我们选取一部分代表性的图像如下：

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task2/full_bank/AccuracyVsMaxDepth_30.png}
        \caption{bank-full dataset}
        \label{fig:AccuracyVsMaxDepth_30 on bank_full dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task2/income/AccuracyVsMaxDepth_30.png}
        \caption{income dataset}
        \label{fig:AccuracyVsMaxDepth_30 on income dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task2/moon/AccuracyVsMaxDepth_30.png}
        \caption{moon dataset}
        \label{fig:AccuracyVsMaxDepth_30 on moon dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task2/allwine/AccuracyVsMaxDepth_30.png}
        \caption{wine-quality dataset}
        \label{fig:AccuracyVsMaxDepth_30 on wine_quality dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task2/digits/AccuracyVsMaxDepth_30.png}
        \caption{digits dataset}
        \label{fig:AccuracyVsMaxDepth_30 on digits dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task2/cancer/AccuracyVsMaxDepth_30.png}
        \caption{breast cancer dataset}
        \label{fig:AccuracyVsMaxDepth_30 on breast cancer dataset}
    \end{minipage}
\end{figure}

\subsection*{2.2 解释}

前三张图片较好的观察到了过拟合现象。而后三张图出现了一些奇怪的现象。个人猜测是图18可能是由于数据集的划分不好（因为我在之后跑的时候发现训练集中的某些特征很少无法5折交叉验证）；图19可能是digits数据集分布比较适合决策树或者是可能还未达到过拟合显示的最大深度（但是由于训练集上的正确率已经稳定达到1.0了所以第二种可能不大）；图20的出现一定程度上说明预剪枝的“贪心”有时候并不好。

\subsection*{Task 3 对决策树算法的未剪枝、预剪枝和后剪枝进行实验比较，并进行适当的统计显著性检验（5 points）} 

\subsection*{3.1 代码}

预剪枝和后剪枝的代码见Prob2/DTmodel.py中，我们使用的K折交叉验证的方式进行实验比较，并选取综合得分最高的作为预剪枝/后剪枝的模型。

\subsection*{3.2 输出结果}

全部结果见Prob2/out/task3文件夹中，我们选取了一部分代表性的结果如下：

\begin{lstlisting}[breaklines=true, language=Python, caption=输出结果]
# adult_income 40k Instances
未剪枝的决策树准确率: 0.811068116209078
预剪枝：
Best params: {'max_depth': 10, 'max_leaf_nodes': 28, 'min_samples_leaf': 1,'min_samples_split': 2}, Best score: 0.855010716612513
预剪枝的决策树准确率: 0.860573674835698
未剪枝 vs 预剪枝 p值: 1.2692101690951682e-78
后剪枝：
Best params: {'ccp_alpha': 0.00013663364519085}, Best score: 0.8599861513035167
后剪枝的决策树准确率: 0.862600577360113
未剪枝 vs 后剪枝 p值: 6.655682551121489e-23

# allwine 4.9K Instances 12 Features
未剪枝的决策树准确率: 0.5902564102564103
预剪枝：
Best params: {'max_depth': 2, 'max_leaf_nodes': 10, 'min_samples_leaf': 1,'min_samples_split': 2}, Best score: 0.5328845851618129
预剪枝的决策树准确率: 0.5056410256410256
未剪枝 vs 预剪枝 p值: 5.281555486312566e-10
后剪枝：
Best params: {'ccp_alpha': 0.007908052002760235}, Best score: 05328845851618129
后剪枝的决策树准确率: 0.5056410256410256
未剪枝 vs 后剪枝 p值: 5.281555486312566e-10

# cancer 569 Instances 30 Features
0.9271835443037976
预剪枝的决策树准确率: 0.9005847953216374
未剪枝 vs 预剪枝 p值: 0.8175564519377121
后剪枝：
Best params: {'ccp_alpha': 0.004745951982132888}, Best score: 09246518987341773
后剪枝的决策树准确率: 0.8947368421052632
未剪枝 vs 后剪枝 p值: 0.9083907372539946
\end{lstlisting}

\subsection*{3.3 个人理解}

在较大的数据集上，剪枝的效果较为明显，而在较小的数据集上，剪枝的效果可能不太明显。

但是我们发现在wine-quality数据集上，预剪枝和后剪枝的结果并不理想，甚至比未剪枝的结果还要差，这可能是由于两种葡萄酒相互是对方的噪声、未经剪枝的决策树恰好具备足够的容量来拟合数据，而剪枝后容量不足，导致性能下降。

此外，在小数据集上，预剪枝和后剪枝的效果可能并不明显。

\subsection*{Task4 请使用PyTorch 或scikit-learn 实现一个简单的多层感知机, 并通过层数、宽度或者优化轮数控制模型复杂度, 实验并比较测试集精度, 讨论并分析观察到的过拟合现象等（5 points）}

我们选择了不同的层数、宽度、轮数的值，并发现在轮数、层数、宽度较大的时候，模型在不少数据集上出现了过拟合现象，全部图像见Prob2/out/task4文件夹中。我们选取一部分代表性的图像如下：

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task4/bank/AccVsLayer_maxe400_layer_1031_175606.png}
        \caption{bank dataset}
        \label{fig:AccVsLayer_maxe400_layer_1031_175606 on bank dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task4/full_bank/AccVsLayer_maxe400_layer_1031_175715.png}
        \caption{full-bank dataset}
        \label{fig:AccVsLayer_maxe400_layer_1031_175715 on full bank dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task4/full_bank/AccVsLayer_maxe400_layer_1031_180102.png}
        \caption{full-bank dataset}
        \label{fig:AccVsLayer_maxe400_layer_1031_180102 on wine_quality dataset}
    \end{minipage}

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task4/full_bank/AccVsLayer_maxe800_layer_1031_182425.png}
        \caption{full-bank dataset}
        \label{fig:AccVsLayer_maxe800_layer_1031_182425 on full-bank dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task4/digits/AccVsLayer_maxe300_layer_1031_175430.png}
        \caption{digits dataset}
        \label{fig:AccVsLayer_maxe300_layer_1031_175430 on digits dataset}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob2/out/task4/car/AccVsLayer_maxe800_layer_1031_175600.png}
        \caption{car-evaluation dataset}
        \label{fig:AccVsLayer_maxe800_layer_1031_175600 on car evaluation dataset}
    \end{minipage}
\end{figure}

\subsection*{2.2 解释}

图21-23可以较好地观察到了因为优化轮数过多、宽度过大、层数过多而导致得过拟合现象，以及轮数小、宽度较低、层数较少时的欠拟合现象。

图24可以观察到，宽度不是越大越好，层数也不是越多越好；

图25可以观察到增加轮数或宽度，可能会导致模型收敛所需的优化轮数变长；

图26可以观察到因为轮数小、宽度较低、层数较少时的欠拟合现象。

\subsection*{2.3 分析}

\begin{itemize}
    \item 层数（Depth）：
    \begin{itemize}
        \item 过多的层数可能导致模型过于复杂，容易记忆训练数据中的噪声，从而引发过拟合，导致在测试集上的性能下降。
        \item 过少的层数可能有欠拟合的风险，但是在我们的数据集出现得不是很明显。
    \end{itemize}
    \item 宽度（Width）：
        \begin{itemize}
            \item 过宽的网络同样会增加模型的复杂度，增加过拟合的可能性，尤其是在训练数据量不足时.
            \item 过窄的网络可能限制模型的表达能力，导致无法充分学习数据中的有用特征。
        \end{itemize}
    \item 优化轮数：
    \begin{itemize}
        \item 训练过多的轮数容易使模型过度拟合训练数据，损害在未见数据上的泛化能力。
        \item 训练轮数过少，模型可能未能充分学习数据特征，导致欠拟合
    \end{itemize}
\end{itemize}

\subsection*{注：数据集选择}

数据集的加载见Prob2中的dataloader.py文件, 数据集本身见data文件夹。主要选择了 adult income, car evaluation, bank marking, dights,wine quality, breast cancer, moon 等数据集。

\vspace{3em}

\section*{三. (20 points) 激活函数比较}
神经网络隐层使用的激活函数一般与输出层不同. 请画出以下几种常见的隐层激活函数的示意图并计算其导数（导数不存在时可指明）, 讨论其优劣（每小题4 points）：

\textcolor{red}{图像见该题目最后}

\begin{enumerate}
    \item Sigmoid 函数, 定义如下
    \begin{equation}
        f(x) = \frac{1}{1 + e^{-x}}.
        \label{eq:sigmoid}
    \end{equation}

    \begin{itemize}
        \item 导数 $f'(x) = f(x)(1 - f(x))$.
        \item 优缺点
        \begin{itemize}
            \item 优点：连续且平滑，输出值范围在 (0, 1) 之间，容易求导。
            \item 缺点：在极大值或极小值处梯度非常小，容易引发梯度消失问题，训练深层网络时表现不佳。
        \end{itemize}
    \end{itemize}

    \item 双曲正切函数 (Hyperbolic Tangent Function, Tanh), 定义如下
    \begin{equation}
        f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}.
        \label{eq:tanh}
    \end{equation}

    \begin{itemize}
        \item 导数 $f'(x) = 1 - f(x)^2$.
        \item 优缺点
        \begin{itemize}
            \item 优点：输出范围为 (-1, 1)，相比于 Sigmoid 函数居中于零，表现更好，避免了 Sigmoid 的零中心问题。
            \item 缺点：在极大值或极小值处梯度非常小，容易引发梯度消失问题，训练深层网络时表现不佳。
        \end{itemize}
    \end{itemize}

    \item 修正线性函数 (Rectified Linear Unit, ReLU) 是近年来最为常用的隐层激活函数之一, 其定义如下
    \begin{equation}
        f(x) = \begin{cases}
            0, & \text{if } x < 0; \\
            x, & \text{otherwise}.
        \end{cases}
        \label{eq:relu}
    \end{equation}

    \begin{itemize}
        \item 导数 $f'(x) = \begin{cases}
            0, & \text{if } x < 0; \\
            1, & \text{if } x > 0; \\
            \text{undefined}, & \text{if } x = 0.
        \end{cases}$
        \item 优缺点
        \begin{itemize}
            \item 优点：计算简单，在正值区域，m导数为常数加速了训练过程，且在正向传播时避免了梯度消失问题，使得深层网络的训练更加高效。
            \item 缺点：存在梯度未定义的情况；且在负值区域梯度为 0，导致神经元“死亡”，即当神经元在负值区域时，它的权重不再更新，可能导致模型不够灵活。
        \end{itemize}
    \end{itemize}

    \item Softplus 函数, 定义如下
    \begin{equation}
        f(x) = \ln(1 + e^x).
        \label{eq:softplus}
    \end{equation}

    \begin{itemize}
        \item 导数 $f'(x) = \frac{1}{1 + e^{-x}} = Sigmoid(x)$.
        \item 优缺点
        \begin{itemize}
            \item 优点：和 ReLU 梯度相似，但其梯度任意点有定义；且在负值区域具有非零的梯度，可以避免 ReLU 的“死亡神经元”问题。
            \item 缺点：相比 ReLU 计算代价较高。
        \end{itemize}
    \end{itemize}

    \item Leaky Rectified Linear Unit (Leaky ReLU) 函数, 定义如下
    \begin{equation}
        f(x) = \begin{cases}
            \alpha x, & \text{if } x < 0; \\
            x, & \text{otherwise}.
        \end{cases}
        \label{eq:leaky_relu}
    \end{equation}
    其中 $\alpha$ 为一个较小的正数, 比如 0.01.

    \begin{itemize}
        \item 导数 $f'(x) = \begin{cases}
            \alpha, & \text{if } x < 0; \\
            1, & \text{if } x > 0; \\
            \text{undefined}, & \text{if } x = 0.
        \end{cases}$
        \item 优缺点
        \begin{itemize}
            \item 优点：和 ReLU 梯度相似，且计算效率高，在负值区域具有非零的梯度，可以避免 ReLU 的“死亡神经元”问题。
            \item 缺点：负值区域的梯度仍然非常小，因此负值区域的学习速度较慢；且仍有未定义的梯度。
        \end{itemize}
    \end{itemize}

\end{enumerate}

\subsection*{函数图像如下：}
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob3/img/f1.png}
        \caption{$f(x) = \frac{1}{1 + e^{-x}}$}
        \label{fig:sigmoid}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob3/img/f2.png}
        \caption{$f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$}
        \label{fig:tanh}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob3/img/f3.png}
        \caption{$f(x) = \ln(1 + e^x)$}
        \label{fig:softplus}
    \end{subfigure}
    \hfill
    \newline
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob3/img/f4.png}
        \caption{
            $f(x) = \begin{cases}
                \alpha x, & \text{if } x < 0; \\
                x, & \text{otherwise}.
            \end{cases}$
        }
        \label{fig:leaky_relu}
    \end{subfigure}
    \hspace{0pt}
    \begin{subfigure}{0.42\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob3/img/f5.png}
        \caption{$f(x) = \begin{cases}
            0, & \text{if } x < 0; \\
            x, & \text{otherwise}.
            \end{cases}$}
        \label{fig:ReLU}
    \end{subfigure}
    \hfill
\end{figure}

\vspace{3em}

\section*{四. (30 points) 神经网络实战}

moons是一个简单的二分类数据集，请实现一个简单的全连接神经网络，并参考教材图5.8实现反向传播算法训练该网络，用于解决二分类问题。
\begin{enumerate}
    \item 使用 NumPy 手动实现神经网络和反向传播算法。（15 points）
    \item 实现并比较不同的权重初始化方法。（5 points）
    \item 在提供的moons数据集上训练网络，观察并分析收敛情况和训练过程。（10 points）
\end{enumerate}

\textbf{提示:}

\begin{enumerate}
    \item 神经网络实现：
\begin{itemize}
    \item 实现一个具有一个隐藏层的全连接神经网络。
    \item 网络结构：输入层(2节点) $\rightarrow$ 隐藏层(4节点) $\rightarrow$ 输出层(1节点)
    \item 隐藏层使用 ReLU 激活函数，输出层使用 Sigmoid 激活函数。
    \item 使用交叉熵损失函数。
\end{itemize}
    \item 权重初始化方法。实现以下三种初始化方法，并比较它们的性能：
\begin{itemize}
    \item 随机初始化：从均匀分布 $U(-0.5, 0.5)$ 中采样。
    \item Xavier 初始化：根据前一层的节点数进行缩放。
    \[ W_{ij} \sim U\left( -\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}} \right) \]

其中 \(n_{in}\) 是前一层的节点数，\(n_{out}\) 是当前层的节点数。
    \item He 初始化：He 初始化假设每一层都是线性的，并且考虑了ReLU激活函数的特性。\[ W_{ij} \sim N\left( 0, \frac{2}{n_{in}} \right) \]

其中 \(n_{in}\) 是前一层的节点数。
\end{itemize}
    \item 训练和分析：
\begin{itemize}
    \item 使用提供的 moons 数据集。
    \item 实现小批量梯度下降算法。
    \item 记录并绘制训练过程中的损失值和准确率，训练结束后绘制决策边界。
    \item 比较不同初始化方法对训练过程和最终性能的影响并给出合理解释。
    \item 尝试不同的学习率，观察其对训练的影响并给出合理解释。
\end{itemize}
\end{enumerate}


\textbf{\large 解:}

\subsection*{4.1 代码}
\begin{itemize}
    \item \textcolor{red}{代码见压缩包code中的Prob4, 详细说明见Prob4中的README文件}; 
    \item \textcolor{red}{详细输出见Prob4中的out文件夹}。
\end{itemize}

\subsection*{4.2 记录并绘制训练过程中的损失值和准确率，训练结束后绘制决策边界}

实验结果在Prob4/out/out.log与info.log中，绘制为表格如下：

\begin{table}[H]
    \centering
    \caption{不同lr,epoch,mehod 的MLP模型在测试集的ACC}
    \begin{tabular}{cccccc}
    \toprule
    \diagbox{lr,epoch}{方法} & random & xavier & he \\
    \midrule
    0.01,500 & 0.87 & 0.87 & 1.0 \\
    \midrule
    0.01,200 & 0.87 & 0.87 & 0.965 \\
    \midrule
    0.1,100 & 0.865 & 0.865 & 1.0 \\
    \midrule
    0.1,40 & 0.88 &  0.88 & 0.995 \\
    \midrule
    1, 50 & 0.86 & 0.865 & 1.0 \\
    \midrule
    1, 20 & 0.875 & 0.895 & 1.0\\
    \bottomrule
    \end{tabular}
\end{table}

所有图片见Prob4/out文件夹中，我们选取了一部分代表性的图片如下：

\begin{figure}[H]
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_random/decision_boundary_epoch-0 before.png}
        \caption{e500 lr0.01 rand init}
        \label{fig:e500 lr0.01 rand init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_random/decision_boundary_test.png}
        \caption{e500 lr0.01 rand test}
        \label{fig:e500 lr0.01 rand test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_random/training_process.png}
        \caption{e500 lr0.01 rand train}
        \label{fig:e500 lr0.01 rand train}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_xavier/decision_boundary_epoch-0 before.png}
        \caption{e500 lr0.01 xavier init}
        \label{fig:e500 lr0.01 xavier init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_xavier/decision_boundary_test.png}
        \caption{e500 lr0.01 xavier test}
        \label{fig:e500 lr0.01 xavier test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_xavier/training_process.png}
        \caption{e500 lr0.01 xavier train}
        \label{fig:e500 lr0.01 xavier train}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_he/decision_boundary_epoch-0 before.png}
        \caption{e500 lr0.01 he init}
        \label{fig:e500 lr0.01 he init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_he/decision_boundary_test.png}
        \caption{e500 lr0.01 he test}
        \label{fig:e500 lr0.01 he test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e500_lr0.01_btz16_he/training_process.png}
        \caption{e500 lr0.01 he train}
        \label{fig:e500 lr0.01 he train}
    \end{minipage}
\end{figure}



\begin{figure}[H]
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_random/decision_boundary_epoch-0 before.png}
        \caption{e100 lr0.1 rand init}
        \label{fig:e100 lr0.1 rand init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_random/decision_boundary_test.png}
        \caption{e100 lr0.1 rand test}
        \label{fig:e100 lr0.1 rand test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_random/training_process.png}
        \caption{e100 lr0.1 rand train}
        \label{fig:e100 lr0.1 rand train}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_xavier/decision_boundary_epoch-0 before.png}
        \caption{e100 lr0.1 xavier init}
        \label{fig:e100 lr0.1 xavier init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_xavier/decision_boundary_test.png}
        \caption{e100 lr0.1 xavier test}
        \label{fig:e100 lr0.1 xavier test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_xavier/training_process.png}
        \caption{e100 lr0.1 xavier train}
        \label{fig:e100 lr0.1 xavier train}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_he/decision_boundary_epoch-0 before.png}
        \caption{e100 lr0.1 he init}
        \label{fig:e100 lr0.1 he init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_he/decision_boundary_test.png}
        \caption{e100 lr0.1 he test}
        \label{fig:e100 lr0.1 he test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e100_lr0.1_btz16_he/training_process.png}
        \caption{e100 lr0.1 he train}
        \label{fig:e100 lr0.1 he train}
    \end{minipage}
\end{figure}



\begin{figure}[H]
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_random/decision_boundary_epoch-0 before.png}
        \caption{e50 lr1.0 rand init}
        \label{fig:e50 lr1.0 rand init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_random/decision_boundary_test.png}
        \caption{e50 lr1.0 rand test}
        \label{fig:e50 lr1.0 rand test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_random/training_process.png}
        \caption{e50 lr1.0 rand train}
        \label{fig:e50 lr1.0 rand train}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_xavier/decision_boundary_epoch-0 before.png}
        \caption{e50 lr1.0 xavier init}
        \label{fig:e50 lr1.0 xavier init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_xavier/decision_boundary_test.png}
        \caption{e50 lr1.0 xavier test}
        \label{fig:e50 lr1.0 xavier test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_xavier/training_process.png}
        \caption{e50 lr1.0 xavier train}
        \label{fig:e50 lr1.0 xavier train}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_he/decision_boundary_epoch-0 before.png}
        \caption{e50 lr1.0 he init}
        \label{fig:e50 lr1.0 he init}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_he/decision_boundary_test.png}
        \caption{e50 lr1.0 he test}
        \label{fig:e50 lr1.0 he test}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Prob4/out/1024_173617/e50_lr1_btz16_he/training_process.png}
        \caption{e50 lr1.0 he train}
        \label{fig:e50 lr1.0 he train}
    \end{minipage}
\end{figure}


\subsection*{4.3 比较不同初始化方法对训练过程和最终性能的影响并给出合理解释}

\textcolor{blue}{4.3.1 结果}

在相同的学习率和训练轮数下，比较了随机初始化、Xavier 初始化和 He 初始化三种不同的权重初始化方法对神经网络模型的训练过程和最终性能的影响。通过实验结果的对比，可以得出以下结论：

在实验环境下，He 初始化方法在 moons 数据集上表现最好，其次是 Xavier 初始化，随机初始化的性能最差。

在学习率小于1的情况下，随机初始化和xavier初始化的收敛速度比He初始化快，但是最终在测试集的表现不如He初始化。

\textcolor{blue}{4.3.2 补充资料}

在查阅相关资料：

\begin{itemize}
    \item 随机初始化 是最基本的方法，适用于浅层网络或作为其他初始化方法的基准。然而，由于没有考虑激活函数和网络层数对输出方差的影响，可能导致信号在传播过程中逐层衰减或爆炸，可能会出现方差不匹配的问题；同时在深层网络中，随机初始化可能导致梯度消失或爆炸，进而影响模型的训练效果和收敛速度。

    \item Xavier 初始化 通过精心设计的方差保持机制，适用于对称激活函数，如 sigmoid 和 tanh。它在深层网络中表现良好，能有效缓解梯度消失和爆炸的问题，促进稳定的训练过程和良好的最终性能。但对于ReLU等非对称激活函数，Xavier初始化没有充分考虑ReLU的特性，可能导致前向传播中部分神经元处于非激活状态，从而影响梯度的传播。
    
    \item He 初始化 进一步优化了 Xavier 初始化，专为 ReLU 激活函数设计。
    \begin{itemize}
        \item 更适合ReLU：ReLU激活函数在正区间线性增长，而在负区间为零，这种非对称性需要更高的初始化方差来保持信号的传播。He初始化通过增加方差，确保在ReLU激活后信号不至于过于衰减。
        \item 有效保持信号传递：在深层网络中，He初始化能够更有效地保持前向传播和反向传播的信号强度，减少梯度消失的问题。
        提升训练效率：由于更好的信号传播，He初始化通常能加快模型的收敛速度，提升最终的模型性能。
    \end{itemize}
    \item 
\end{itemize}

\textcolor{blue}{4.3.3 个人解释}

实现的MLP中，虽然Loss层使用的是sigmoid函数，但是隐藏层使用的是ReLU函数，因此He初始化在这种情况下表现最好。而由于Xavier初始化没有考虑ReLU的特性，但是适合sigmoid对称激活函数，因此在这种情况下表现次之。随机初始化则表现最差。

\subsection*{4.4 尝试不同的学习率，观察其对训练的影响并给出合理解释。}

在训练神经网络模型时，学习率（Learning Rate）是一个关键的超参数，决定了优化算法在参数空间中移动的步伐大小。选择合适的学习率对于模型的收敛速度和最终性能具有重要影响。

为了研究学习率对训练的影响，选择了0.01, 0.1, 1.0这几种不同的学习率值进行实验.

\textcolor{blue}{4.4.1 结果}
\begin{itemize}
    \item \textbf{学习率较低（0.01）}：导致模型收敛速度较慢，训练效率低下，需要更多的训练轮数(这里是500轮)才能达到较好的性能。
    \item \textbf{学习率适中（0.1）}：在保证训练稳定性的同时，加快了模型的收敛速度，提高了训练效率。
    \item \textbf{学习率过高（1）}：在上图中，明显发现lr=1.0时，在Xavier初始化和随机初始化的条件下，引发训练过程的不稳定，损失函数和训练集上的正确率出现剧烈波动，导致模型无法收敛。
\end{itemize}

\textcolor{blue}{4.4.2 合理解释}

学习率决定了每次参数更新的步长大小。当学习率过低时，优化过程需要更多的迭代次数才能接近最优解，训练效率低。而学习率过高时，优化过程可能在最优解附近来回跳跃，可能无法稳定收敛，甚至导致参数发散，影响模型性能。

\end{document}
